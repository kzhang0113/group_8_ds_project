{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Group 8 Project - User Knowledge Modeling Data Set**\n",
    "Authors: Minting Fu, Zeti Batrisha Jamiluddin Amini, Liz Ji, Caroline Zhang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Developing the ability to gain a deep understanding in particular subjects has been a major goal of both students and educational institutions. This calls for the question: what are some strategies that can help us enhance such understanding? In this study, we aim to explore the linkage between knowledge level and some of its potential contributors, by analyzing the <a href='https://archive.ics.uci.edu/ml/datasets/User+Knowledge+Modeling'>User Knowledge dataset</a>. This dataset contains 403 students' knowledge levels in the subject of Electrical DC Machines, and it has the following variables:\n",
    "\n",
    "* STG : the degree of study time for goal object materails.\n",
    "* SCG : the degree of repetition number of user for goal object materails.\n",
    "* STR : the degree of study time of user for related objects with goal object.\n",
    "* LPR : the exam performance of user for related objects with goal object.\n",
    "* PEG : the exam performance of user for goal objects.\n",
    "* UNS : the user knowledge level for goal objects.\n",
    "\n",
    "\n",
    "We will use this dataset to investigate the following question:\n",
    "* Is there a relationship between STG, SCG, STR, LPR, PEG and UNS? (ie. Which of STG, SCG, STR, LPR, PEG are contributing factors to UNS and can we predict UNS based on them?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we do is to import the pacakges we are going to use for the entire project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load extra R packages we need\n",
    "library(readxl)\n",
    "library(tidyverse)\n",
    "library(repr)\n",
    "library(tidymodels)\n",
    "library(GGally)\n",
    "options(repr.matrix.max.rows = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the set.seed function to make sure our code is reproducible\n",
    "set.seed(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download and read the dataset. Since the dataset has been seperated into training and testing data ahead, we will read two datasets seperatedly as `training_data` and `testing_data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A tibble: 258 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>STG</th><th scope=col>SCG</th><th scope=col>STR</th><th scope=col>LPR</th><th scope=col>PEG</th><th scope=col>UNS</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>very_low</td></tr>\n",
       "\t<tr><td>0.08</td><td>0.08</td><td>0.10</td><td>0.24</td><td>0.90</td><td>High    </td></tr>\n",
       "\t<tr><td>0.06</td><td>0.06</td><td>0.05</td><td>0.25</td><td>0.33</td><td>Low     </td></tr>\n",
       "\t<tr><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td></tr>\n",
       "\t<tr><td>0.54</td><td>0.82</td><td>0.71</td><td>0.29</td><td>0.77</td><td>High  </td></tr>\n",
       "\t<tr><td>0.50</td><td>0.75</td><td>0.81</td><td>0.61</td><td>0.26</td><td>Middle</td></tr>\n",
       "\t<tr><td>0.66</td><td>0.90</td><td>0.76</td><td>0.87</td><td>0.74</td><td>High  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 258 × 6\n",
       "\\begin{tabular}{llllll}\n",
       " STG & SCG & STR & LPR & PEG & UNS\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <chr>\\\\\n",
       "\\hline\n",
       "\t 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & very\\_low\\\\\n",
       "\t 0.08 & 0.08 & 0.10 & 0.24 & 0.90 & High    \\\\\n",
       "\t 0.06 & 0.06 & 0.05 & 0.25 & 0.33 & Low     \\\\\n",
       "\t ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮\\\\\n",
       "\t 0.54 & 0.82 & 0.71 & 0.29 & 0.77 & High  \\\\\n",
       "\t 0.50 & 0.75 & 0.81 & 0.61 & 0.26 & Middle\\\\\n",
       "\t 0.66 & 0.90 & 0.76 & 0.87 & 0.74 & High  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 258 × 6\n",
       "\n",
       "| STG &lt;dbl&gt; | SCG &lt;dbl&gt; | STR &lt;dbl&gt; | LPR &lt;dbl&gt; | PEG &lt;dbl&gt; | UNS &lt;chr&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | very_low |\n",
       "| 0.08 | 0.08 | 0.10 | 0.24 | 0.90 | High     |\n",
       "| 0.06 | 0.06 | 0.05 | 0.25 | 0.33 | Low      |\n",
       "| ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |\n",
       "| 0.54 | 0.82 | 0.71 | 0.29 | 0.77 | High   |\n",
       "| 0.50 | 0.75 | 0.81 | 0.61 | 0.26 | Middle |\n",
       "| 0.66 | 0.90 | 0.76 | 0.87 | 0.74 | High   |\n",
       "\n"
      ],
      "text/plain": [
       "    STG  SCG  STR  LPR  PEG  UNS     \n",
       "1   0.00 0.00 0.00 0.00 0.00 very_low\n",
       "2   0.08 0.08 0.10 0.24 0.90 High    \n",
       "3   0.06 0.06 0.05 0.25 0.33 Low     \n",
       "⋮   ⋮    ⋮    ⋮    ⋮    ⋮    ⋮       \n",
       "256 0.54 0.82 0.71 0.29 0.77 High    \n",
       "257 0.50 0.75 0.81 0.61 0.26 Middle  \n",
       "258 0.66 0.90 0.76 0.87 0.74 High    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# download the file from the website\n",
    "url <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/00257/Data_User_Modeling_Dataset_Hamdi%20Tolga%20KAHRAMAN.xls'\n",
    "download.file(url, destfile='data/user_knowledge_data.xls')\n",
    "\n",
    "# read the training dataset\n",
    "training_data <- read_excel('data/user_knowledge_data.xls', sheet=2, range='A1:F259')           \n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table1. Raw training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A tibble: 145 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>STG</th><th scope=col>SCG</th><th scope=col>STR</th><th scope=col>LPR</th><th scope=col>PEG</th><th scope=col>UNS</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.00</td><td>0.10</td><td>0.50</td><td>0.26</td><td>0.05</td><td>Very Low</td></tr>\n",
       "\t<tr><td>0.05</td><td>0.05</td><td>0.55</td><td>0.60</td><td>0.14</td><td>Low     </td></tr>\n",
       "\t<tr><td>0.08</td><td>0.18</td><td>0.63</td><td>0.60</td><td>0.85</td><td>High    </td></tr>\n",
       "\t<tr><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td></tr>\n",
       "\t<tr><td>0.56</td><td>0.60</td><td>0.77</td><td>0.13</td><td>0.32</td><td>Low   </td></tr>\n",
       "\t<tr><td>0.66</td><td>0.68</td><td>0.81</td><td>0.57</td><td>0.57</td><td>Middle</td></tr>\n",
       "\t<tr><td>0.68</td><td>0.64</td><td>0.79</td><td>0.97</td><td>0.24</td><td>Middle</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 145 × 6\n",
       "\\begin{tabular}{llllll}\n",
       " STG & SCG & STR & LPR & PEG & UNS\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <chr>\\\\\n",
       "\\hline\n",
       "\t 0.00 & 0.10 & 0.50 & 0.26 & 0.05 & Very Low\\\\\n",
       "\t 0.05 & 0.05 & 0.55 & 0.60 & 0.14 & Low     \\\\\n",
       "\t 0.08 & 0.18 & 0.63 & 0.60 & 0.85 & High    \\\\\n",
       "\t ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮\\\\\n",
       "\t 0.56 & 0.60 & 0.77 & 0.13 & 0.32 & Low   \\\\\n",
       "\t 0.66 & 0.68 & 0.81 & 0.57 & 0.57 & Middle\\\\\n",
       "\t 0.68 & 0.64 & 0.79 & 0.97 & 0.24 & Middle\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 145 × 6\n",
       "\n",
       "| STG &lt;dbl&gt; | SCG &lt;dbl&gt; | STR &lt;dbl&gt; | LPR &lt;dbl&gt; | PEG &lt;dbl&gt; | UNS &lt;chr&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| 0.00 | 0.10 | 0.50 | 0.26 | 0.05 | Very Low |\n",
       "| 0.05 | 0.05 | 0.55 | 0.60 | 0.14 | Low      |\n",
       "| 0.08 | 0.18 | 0.63 | 0.60 | 0.85 | High     |\n",
       "| ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |\n",
       "| 0.56 | 0.60 | 0.77 | 0.13 | 0.32 | Low    |\n",
       "| 0.66 | 0.68 | 0.81 | 0.57 | 0.57 | Middle |\n",
       "| 0.68 | 0.64 | 0.79 | 0.97 | 0.24 | Middle |\n",
       "\n"
      ],
      "text/plain": [
       "    STG  SCG  STR  LPR  PEG  UNS     \n",
       "1   0.00 0.10 0.50 0.26 0.05 Very Low\n",
       "2   0.05 0.05 0.55 0.60 0.14 Low     \n",
       "3   0.08 0.18 0.63 0.60 0.85 High    \n",
       "⋮   ⋮    ⋮    ⋮    ⋮    ⋮    ⋮       \n",
       "143 0.56 0.60 0.77 0.13 0.32 Low     \n",
       "144 0.66 0.68 0.81 0.57 0.57 Middle  \n",
       "145 0.68 0.64 0.79 0.97 0.24 Middle  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read the testing dataset\n",
    "testing_data <- read_excel('data/user_knowledge_data.xls', sheet=3, range='A1:F146')\n",
    "testing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table2. Raw testing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the two datasets above, they are almost clean. There are few things we do to clean the data:\n",
    "\n",
    "1. Our target variable `UNS` is in chr type, so we need to change the type of the `UNS` to factor type in order to do further analysis. \n",
    "2. We notice that the labels of `UNS` in training set are different from those in testing set. They are lower case letter with underscores in the training set, whereas they are all upper case letter in the testing set. \n",
    "3. We will change the `UNS` level's order in the training data to `very_low`, `low`, `high`, and `middle` so that the `UNS` level will be consistent between both training and testing datasets. \n",
    "3. We will check whether there are missing data in two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A tibble: 258 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>STG</th><th scope=col>SCG</th><th scope=col>STR</th><th scope=col>LPR</th><th scope=col>PEG</th><th scope=col>UNS</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>very_low</td></tr>\n",
       "\t<tr><td>0.08</td><td>0.08</td><td>0.10</td><td>0.24</td><td>0.90</td><td>high    </td></tr>\n",
       "\t<tr><td>0.06</td><td>0.06</td><td>0.05</td><td>0.25</td><td>0.33</td><td>low     </td></tr>\n",
       "\t<tr><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td></tr>\n",
       "\t<tr><td>0.54</td><td>0.82</td><td>0.71</td><td>0.29</td><td>0.77</td><td>high  </td></tr>\n",
       "\t<tr><td>0.50</td><td>0.75</td><td>0.81</td><td>0.61</td><td>0.26</td><td>middle</td></tr>\n",
       "\t<tr><td>0.66</td><td>0.90</td><td>0.76</td><td>0.87</td><td>0.74</td><td>high  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 258 × 6\n",
       "\\begin{tabular}{llllll}\n",
       " STG & SCG & STR & LPR & PEG & UNS\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <fct>\\\\\n",
       "\\hline\n",
       "\t 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & very\\_low\\\\\n",
       "\t 0.08 & 0.08 & 0.10 & 0.24 & 0.90 & high    \\\\\n",
       "\t 0.06 & 0.06 & 0.05 & 0.25 & 0.33 & low     \\\\\n",
       "\t ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮\\\\\n",
       "\t 0.54 & 0.82 & 0.71 & 0.29 & 0.77 & high  \\\\\n",
       "\t 0.50 & 0.75 & 0.81 & 0.61 & 0.26 & middle\\\\\n",
       "\t 0.66 & 0.90 & 0.76 & 0.87 & 0.74 & high  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 258 × 6\n",
       "\n",
       "| STG &lt;dbl&gt; | SCG &lt;dbl&gt; | STR &lt;dbl&gt; | LPR &lt;dbl&gt; | PEG &lt;dbl&gt; | UNS &lt;fct&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | very_low |\n",
       "| 0.08 | 0.08 | 0.10 | 0.24 | 0.90 | high     |\n",
       "| 0.06 | 0.06 | 0.05 | 0.25 | 0.33 | low      |\n",
       "| ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |\n",
       "| 0.54 | 0.82 | 0.71 | 0.29 | 0.77 | high   |\n",
       "| 0.50 | 0.75 | 0.81 | 0.61 | 0.26 | middle |\n",
       "| 0.66 | 0.90 | 0.76 | 0.87 | 0.74 | high   |\n",
       "\n"
      ],
      "text/plain": [
       "    STG  SCG  STR  LPR  PEG  UNS     \n",
       "1   0.00 0.00 0.00 0.00 0.00 very_low\n",
       "2   0.08 0.08 0.10 0.24 0.90 high    \n",
       "3   0.06 0.06 0.05 0.25 0.33 low     \n",
       "⋮   ⋮    ⋮    ⋮    ⋮    ⋮    ⋮       \n",
       "256 0.54 0.82 0.71 0.29 0.77 high    \n",
       "257 0.50 0.75 0.81 0.61 0.26 middle  \n",
       "258 0.66 0.90 0.76 0.87 0.74 high    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# unite value format and change the UNS type and order\n",
    "training_data$UNS = tolower(training_data$UNS)\n",
    "training_data <- training_data %>%\n",
    "                 mutate(UNS = factor(UNS, levels = c(\"very_low\", \"low\", \"high\", \"middle\")))\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table3. Clean and tidy training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A tibble: 145 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>STG</th><th scope=col>SCG</th><th scope=col>STR</th><th scope=col>LPR</th><th scope=col>PEG</th><th scope=col>UNS</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.00</td><td>0.10</td><td>0.50</td><td>0.26</td><td>0.05</td><td>very_low</td></tr>\n",
       "\t<tr><td>0.05</td><td>0.05</td><td>0.55</td><td>0.60</td><td>0.14</td><td>low     </td></tr>\n",
       "\t<tr><td>0.08</td><td>0.18</td><td>0.63</td><td>0.60</td><td>0.85</td><td>high    </td></tr>\n",
       "\t<tr><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td></tr>\n",
       "\t<tr><td>0.56</td><td>0.60</td><td>0.77</td><td>0.13</td><td>0.32</td><td>low   </td></tr>\n",
       "\t<tr><td>0.66</td><td>0.68</td><td>0.81</td><td>0.57</td><td>0.57</td><td>middle</td></tr>\n",
       "\t<tr><td>0.68</td><td>0.64</td><td>0.79</td><td>0.97</td><td>0.24</td><td>middle</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 145 × 6\n",
       "\\begin{tabular}{llllll}\n",
       " STG & SCG & STR & LPR & PEG & UNS\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <fct>\\\\\n",
       "\\hline\n",
       "\t 0.00 & 0.10 & 0.50 & 0.26 & 0.05 & very\\_low\\\\\n",
       "\t 0.05 & 0.05 & 0.55 & 0.60 & 0.14 & low     \\\\\n",
       "\t 0.08 & 0.18 & 0.63 & 0.60 & 0.85 & high    \\\\\n",
       "\t ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮\\\\\n",
       "\t 0.56 & 0.60 & 0.77 & 0.13 & 0.32 & low   \\\\\n",
       "\t 0.66 & 0.68 & 0.81 & 0.57 & 0.57 & middle\\\\\n",
       "\t 0.68 & 0.64 & 0.79 & 0.97 & 0.24 & middle\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 145 × 6\n",
       "\n",
       "| STG &lt;dbl&gt; | SCG &lt;dbl&gt; | STR &lt;dbl&gt; | LPR &lt;dbl&gt; | PEG &lt;dbl&gt; | UNS &lt;fct&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| 0.00 | 0.10 | 0.50 | 0.26 | 0.05 | very_low |\n",
       "| 0.05 | 0.05 | 0.55 | 0.60 | 0.14 | low      |\n",
       "| 0.08 | 0.18 | 0.63 | 0.60 | 0.85 | high     |\n",
       "| ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |\n",
       "| 0.56 | 0.60 | 0.77 | 0.13 | 0.32 | low    |\n",
       "| 0.66 | 0.68 | 0.81 | 0.57 | 0.57 | middle |\n",
       "| 0.68 | 0.64 | 0.79 | 0.97 | 0.24 | middle |\n",
       "\n"
      ],
      "text/plain": [
       "    STG  SCG  STR  LPR  PEG  UNS     \n",
       "1   0.00 0.10 0.50 0.26 0.05 very_low\n",
       "2   0.05 0.05 0.55 0.60 0.14 low     \n",
       "3   0.08 0.18 0.63 0.60 0.85 high    \n",
       "⋮   ⋮    ⋮    ⋮    ⋮    ⋮    ⋮       \n",
       "143 0.56 0.60 0.77 0.13 0.32 low     \n",
       "144 0.66 0.68 0.81 0.57 0.57 middle  \n",
       "145 0.68 0.64 0.79 0.97 0.24 middle  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# unite value format and change the UNS type and order\n",
    "testing_data$UNS = tolower(testing_data$UNS)\n",
    "testing_data <- testing_data %>%\n",
    "    mutate(UNS = str_replace(UNS, \" \", \"_\")) %>%\n",
    "    mutate(UNS = as_factor(UNS))\n",
    "testing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table4. Clean and tidy testing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now the labels of UNS in training set are the same with those in testing set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'The number of missing data in training data: 0'"
      ],
      "text/latex": [
       "'The number of missing data in training data: 0'"
      ],
      "text/markdown": [
       "'The number of missing data in training data: 0'"
      ],
      "text/plain": [
       "[1] \"The number of missing data in training data: 0\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'The number of missing data in testing data: 0'"
      ],
      "text/latex": [
       "'The number of missing data in testing data: 0'"
      ],
      "text/markdown": [
       "'The number of missing data in testing data: 0'"
      ],
      "text/plain": [
       "[1] \"The number of missing data in testing data: 0\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test missing values\n",
    "sprintf(\"The number of missing data in training data: %s\", sum(is.na(training_data)))\n",
    "sprintf(\"The number of missing data in testing data: %s\", sum(is.na(testing_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values in our training and testing datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can see from each table above:\n",
    "\n",
    "  1. Each row is a single observation\n",
    "  2. Each column is a single variable \n",
    "  3. Each value is a single cell\n",
    "  \n",
    "Therefore, the training set and testing set are clean and tidy now.\n",
    "\n",
    "We also notice that the proportion of the training data is around 64%, and the proportion of the test data is around 36% from calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to summarize the training dataset information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A tibble: 1 × 5</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>STG</th><th scope=col>SCG</th><th scope=col>STR</th><th scope=col>LPR</th><th scope=col>PEG</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.99</td><td>0.9</td><td>0.95</td><td>0.99</td><td>0.93</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 1 × 5\n",
       "\\begin{tabular}{lllll}\n",
       " STG & SCG & STR & LPR & PEG\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 0.99 & 0.9 & 0.95 & 0.99 & 0.93\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 1 × 5\n",
       "\n",
       "| STG &lt;dbl&gt; | SCG &lt;dbl&gt; | STR &lt;dbl&gt; | LPR &lt;dbl&gt; | PEG &lt;dbl&gt; |\n",
       "|---|---|---|---|---|\n",
       "| 0.99 | 0.9 | 0.95 | 0.99 | 0.93 |\n",
       "\n"
      ],
      "text/plain": [
       "  STG  SCG STR  LPR  PEG \n",
       "1 0.99 0.9 0.95 0.99 0.93"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarise the maximum value of each predictors\n",
    "training_data_max <- training_data %>%\n",
    "                     select(-UNS) %>%\n",
    "                     map_df(max)\n",
    "training_data_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table5. The mamximum value of each predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A tibble: 1 × 5</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>STG</th><th scope=col>SCG</th><th scope=col>STR</th><th scope=col>LPR</th><th scope=col>PEG</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 1 × 5\n",
       "\\begin{tabular}{lllll}\n",
       " STG & SCG & STR & LPR & PEG\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 0 & 0 & 0 & 0 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 1 × 5\n",
       "\n",
       "| STG &lt;dbl&gt; | SCG &lt;dbl&gt; | STR &lt;dbl&gt; | LPR &lt;dbl&gt; | PEG &lt;dbl&gt; |\n",
       "|---|---|---|---|---|\n",
       "| 0 | 0 | 0 | 0 | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "  STG SCG STR LPR PEG\n",
       "1 0   0   0   0   0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarise the minimum value of each predictors\n",
    "training_data_min <- training_data %>%\n",
    "                     select(-UNS) %>%\n",
    "                     map_df(min)\n",
    "training_data_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table6. The minimum value of each predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A tibble: 1 × 5</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>STG</th><th scope=col>SCG</th><th scope=col>STR</th><th scope=col>LPR</th><th scope=col>PEG</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.3711473</td><td>0.3556744</td><td>0.4680039</td><td>0.4327132</td><td>0.4585388</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 1 × 5\n",
       "\\begin{tabular}{lllll}\n",
       " STG & SCG & STR & LPR & PEG\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 0.3711473 & 0.3556744 & 0.4680039 & 0.4327132 & 0.4585388\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 1 × 5\n",
       "\n",
       "| STG &lt;dbl&gt; | SCG &lt;dbl&gt; | STR &lt;dbl&gt; | LPR &lt;dbl&gt; | PEG &lt;dbl&gt; |\n",
       "|---|---|---|---|---|\n",
       "| 0.3711473 | 0.3556744 | 0.4680039 | 0.4327132 | 0.4585388 |\n",
       "\n"
      ],
      "text/plain": [
       "  STG       SCG       STR       LPR       PEG      \n",
       "1 0.3711473 0.3556744 0.4680039 0.4327132 0.4585388"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarise the average value of each predictors\n",
    "training_data_avg <- training_data %>%\n",
    "                     select(-UNS) %>%\n",
    "                     map_df(mean)\n",
    "training_data_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table7. The average value of each predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the tables above, we can see the range of each variable is :\n",
    "* The range of STG, the degree of study time for goal object materails, is [0, 0.99].\n",
    "* The range of SCG, the degree of repetition number of user for goal object materails, is [0, 0.9].\n",
    "* The range of STR, the degree of study time of user for related objects with goal object, is [0, 0.95].\n",
    "* The range of LPR, the exam performance of user for related objects with goal object, is [0, 0.99].\n",
    "* The range of PEG, the exam performance of user for goal objects, is [0, 0.93]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have some basic ideas about our dataset. In the next step, We want to know which factor(s) is related to our target variable `UNS`, in other words, which factor is our explanatory variable. To do this, we need to visualize our data to find out if there is a relationship between `STG`, `SCG`, `STR`, `LPR`, `PEG` and `UNS`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "To visualize the data, we will use the `ggpairs` function, which returns a matrix of plots for a given dataset. Since we have 5 potential explanatory variables and 1 target variable, it is better to compare the distibution and evaluate the association of them in a whole. The `ggpairs` function provides an efficient way to exploring the distribution and correlation between different variables.\n",
    "\n",
    "The `columns` argument is used to select the number of columns we want to include in the plot. In this case, we need to put `1:5` here since we have 5 explanatory variables in total. \n",
    "\n",
    "We also need to change the font size of correlation values in order to make it fit in the panel and readable. To change the font size, we need to include `upper = list(continuous = wrap('cor', size = ...)` in our `ggpairs` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the matrix of plot by using ggpairs function\n",
    "options(repr.plot.width = 14, repr.plot.height = 12)\n",
    "relationship_plot <- ggpairs(training_data, \n",
    "                             columns = 1:5, \n",
    "                             ggplot2 :: aes(color = UNS), \n",
    "                             upper = list(continuous = wrap('cor', size = 5))) +\n",
    "                             labs(caption = \"Figure1. The Scatterplot matrix of `user knowledge dataset` using ggpairs function.\") +\n",
    "                             theme(plot.caption.position = \"plot\", \n",
    "                                   plot.caption = element_text(size = 17, hjust =0))\n",
    "relationship_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this graph, the diagnal shows the density plot of quantitative variables. The lower left half of the grids shows the scatterplot between each two of the five quantitative variables. The upper panel shows the correlation coefficient. It describes the degree of linear relationships between each two of the five quantitative variables (all variables beside UNS). Since this is not relevant to their relationships with UNS, we will not use this part of the graph to determine predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from previous information, we have 6 variables in this user knowledge dataset, they are:\n",
    "\n",
    "* STG : the degree of study time for goal object materails. (Quantitative variable)\n",
    "* SCG : the degree of repetition number of user for goal object materails. (Quantitative variable)\n",
    "* STR : the degree of study time of user for related objects with goal object. (Quantitative variable)\n",
    "* LPR : the exam performance of user for related objects with goal object. (Quantitative variable)\n",
    "* PEG : the exam performance of user for goal objects. (Quantitative variable)\n",
    "* UNS : the knowledge level of users.(Categorical variable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we look at the density plots on the diagnal, which represent the distributions of each quantitative variable, classified by UNS. Among all of them, the one representing PEG has the most distinct color groups and the least overlapping between different color groups, meaning different ranges of PEG values clearly correspond to different UNS classifications. To observe the distribution of PEG by different UNS classifications in more detail, we create the histogram below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the histogram of peg\n",
    "peg <- training_data %>%\n",
    "    ggplot(aes(x = PEG, fill = UNS)) +\n",
    "    geom_histogram(bins=30, alpha = 0.6, position = \"identity\") +\n",
    "    labs(x = \"PEG\", \n",
    "         y = \"Count\", \n",
    "         fill = \"User knowledge levels\", \n",
    "         title = \"PEG Distribution\", \n",
    "         caption = \"Figure2. The histogram of `PEG`\") +\n",
    "    theme(text = element_text(size = 20), \n",
    "          plot.caption = element_text(size = 17, hjust = 0), \n",
    "          plot.title = element_text(hjust = 0.5)) +\n",
    "    scale_fill_brewer(palette = \"PuOr\")\n",
    "peg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the histogram, we can see that there are 4 disctinct regions represents 4 different user knowledge levels:\n",
    "\n",
    "* The interval of \"very_low knowledge level\" is around [0.00, 0.25].\n",
    "* The interval of \"low knowledge level\" is around [0.00, 0.38].\n",
    "* The interval of \"middle knowledge level\" is around [0.25, 0.65].\n",
    "* The interval of \"high knowledge level\" is around [0.63, 0.90]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 4 different user knowledge levles are classified into 4 different regions by PEG, **PEG is a good predictor to predict UNS**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to Figure 1. We mentioned that the density plot of PEG has very nice and distinct groupings. Another good alternative density plot is the LPR one. Although there are some overlapping areas on the plot, we can still see 4 relatively distinct coloring group compared to those of the rest variables (STG, SCG, and STR), where all colors almost overlap on top of each other.\n",
    "\n",
    "To confirm that LPR and PEG are good predictors, we then look at the scatterplots in the lower panel. For all of the scatterplots except for the one between PEG and LPR, we can see that the points are very dispersed, and points of different colors are all mixed together. Only for the scatterplot between PEG and LPR, there are clearly four distinct clusters. This indicates that LPR, PEG and UNS are correlated to each other.\n",
    "\n",
    "These observations remind us that **LPR may be the next good predictor that we should consider on.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addiditon, LPR represents the exam performance of user for related objects with goal object, PEG represents the exam performance of user for goal objects. By our common sense, it's never too much to learn. Also, the more knowledge and skills students study, the better performance they will get in an exam. This idea has been demonstrated by Hartwig, Was, Isaacson and Dunlosky (2011) in their study that a student's general knowledge level (i.e., obtaining materials that related to and beyond class content) has a strong positive relationship with the exam performance. Obtaining high grades on exams regarding both goal objects and  related objects likely indicates a student's high knowledge level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, combining our analysis of the density plots and scatterplots in Figure 1, as well as the study of Hartwig, Was, Isaacson and Dunlosky, we can reason that **we are going to use the LPR (the exam performance of uses for related objects with goal object) and PEG (the exam performance of user for goal objects) to predict the UNS (the knowledge level of users).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using 2 quantitative variables(LPR and PEG) to predict a categorical variable(UNS), we will use the `k-nearest neighbors classification algorithm` to analyze this dataset. The detailed process and commenting are as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data analysis with PEG and LPR as the predictors for UNS\n",
    "\n",
    "# create a recipe\n",
    "data_recipe <- recipe(UNS ~ PEG + LPR, data = training_data) %>%\n",
    "               step_scale(all_predictors()) %>%\n",
    "               step_center(all_predictors())\n",
    "\n",
    "# create k-nearest-neighbor model specification\n",
    "knn_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) %>%\n",
    "            set_engine(\"kknn\") %>%\n",
    "            set_mode(\"classification\")\n",
    "\n",
    "# perform 5-fold cross-validation\n",
    "data_vfold <- vfold_cv(training_data, v = 5, strata = UNS)\n",
    "\n",
    "# specify k_vals to tune\n",
    "k_vals <- tibble(neighbors = seq(from = 1, to = 10))\n",
    "\n",
    "# get the results\n",
    "data_results <- workflow() %>%\n",
    "                add_recipe(data_recipe) %>%\n",
    "                add_model(knn_spec) %>%\n",
    "                tune_grid(resamples = data_vfold, grid = k_vals) %>%\n",
    "                collect_metrics()\n",
    "\n",
    "# filter accuracy\n",
    "accuracies <- data_results %>%\n",
    "              filter(.metric == \"accuracy\")\n",
    "\n",
    "# plot the accuracy versus k\n",
    "accuracy_plot <- ggplot(accuracies, aes(x = neighbors, y = mean)) +\n",
    "                 geom_point() +\n",
    "                 geom_line() +\n",
    "                 labs(x = \"Neighbors\", \n",
    "                      y = \"Accuracy Estimation\", \n",
    "                      caption = \"Figure3. Plot of estimated accuracy vs the number of neighbors.\") +\n",
    "                 theme(text = element_text(size = 20), \n",
    "                       plot.caption.position = \"plot\", \n",
    "                       plot.caption = element_text(size = 17, hjust = 0)) +\n",
    "                 scale_x_continuous(breaks = 1:10)\n",
    "accuracy_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the Figure3, the number of neighbors, k = 5 or k = 6 provides the highest accuracy (around 96.35%). Furthermore, estimated accuracy changes by a large amount if we increase or decrease k near k = 5 or k = 6.  And, both k = 5 and k = 6 do not create a prohibitively expensive computational cost of training. However, 5 is an odd number which can avoid the risk of even number ties. Considering all of these four aspects, we would like to select k = 5 for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the new knn classifier with the \"best\" of k\n",
    "new_knn_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = 5) %>%\n",
    "                set_engine(\"kknn\") %>%\n",
    "                set_mode(\"classification\")\n",
    "\n",
    "# create a new wokflow\n",
    "knn_fit <- workflow() %>%\n",
    "           add_recipe(data_recipe) %>%\n",
    "           add_model(new_knn_spec) %>%\n",
    "           fit(data = training_data)\n",
    "\n",
    "# evaluate our model by testing set\n",
    "testing_data_predictions <- predict(knn_fit, testing_data) %>%\n",
    "                            bind_cols(testing_data)\n",
    "testing_data_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table8. The dataframe with an extra .pred_class column added to the original testing data. \n",
    "\n",
    "The `UNS` variable contains the true knowledge level of a user, while the `.pred_class` contains the predicted knowledge level of a user from the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the accuracy of our model\n",
    "test_prediction <- testing_data_predictions %>% \n",
    "                   metrics(truth = UNS, estimate = .pred_class) %>%\n",
    "                   filter(.metric == \"accuracy\")\n",
    "test_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table9. The accuracy of the classifier. \n",
    "\n",
    "The value of the `.estimate` variable shows that the estimated accuracy of our classifier on the testing data is 97.24%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the confusion matrix\n",
    "test_confusion <- testing_data_predictions %>%\n",
    "                  conf_mat(truth = UNS, estimate = .pred_class)\n",
    "test_confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table10. The confusion matrix for the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows 24 observations were correctly predicted as very_low, 45 observations were correctly predicted as low, 39 observations were correctly predicted as high, and 33 observations were correctly predicted as middle. Therefore, the classifier labeled 24 + 45 + 39 + 33 = 141 observations correctly. However, it also indicates that the classifier made some small mistakes, in which, it classified 2 observations as low when they were truly very_low, 1 observations as low when they were truly middle, and 1 observations as middle when they were truly low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What we find?\n",
    "We firstly find that LPR and PEG are the 2 predictors we decide to use in predicting target variable UNS, since both of them show more distinct grouping in scatterplots as well as density plots regarding to UNS. \n",
    "\n",
    "Then, we find the optimal value of k is 5 using cross-validation. Since when k = 5, the highest estimated accuracy(around 96.35%) is reached, estimated accuracy changes by only a small amount if we increase or decrease the value of k = 5, k = 5 does not create a prohibitively expensive computational cost of training and 5 is an odd number which avoids the risk of even number ties. \n",
    "\n",
    "Finaly, we find the accuracy of our model is around 97.24% using k nearest neighbor classification model with 2 predictors (LPR and UNS).  From the confusion matrix, we see that our model only makes small mistakes when classifying between knowledge levels like very_low versus low, and low versus middle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of expectations of this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--What are our expectations?\n",
    "\n",
    "1. We expected to find a positive relationship between PEG, STG, and UNS, and we planned to use PEG and STG to predict UNS.\n",
    "\n",
    "2. We expected that our k-nearest neighbos classification model with 2 predictors, `LPR` (the exam performance of user for related objects with goal object) and `PEG`(the exam performance of user for goal objects), will be able to predict the `UNS` (the knowledge level of users) with a high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--Is this what we expected to find?\n",
    "\n",
    "1. The expectation of picking predictor variables are different from our findings. According to the Figure1 and Figure2, the distinct coloring groups on the scatterplots and the histogram of LPR regarding to the UNS and that of PEG regarding to the UNS indicates LPR and PEG are better predictors rather than STG(the degree of study time for goal object materails). An interpretation of this inconsistency is that different individuals take different amount of study time. For example, a genius does not need to study for a long time to get a higher exam performance.\n",
    "\n",
    "2. The expectation of the accuracy of our model is consistent with our result, it achieves a high accuracy around 97.24%. This indicates that our k-nearest-neighbor classification model might be a good one to predict the knowledge level of users. We got a relatively high accuracy of the model may because the 2 predictors (`LPR` and `PEG`) we choose may strongly correlated with target variable `UNS`, which is shown on the scatterplots and density plots Figure1 and Figure2. Also, we uses the cross-validation and finds the optimal value of k, the plot of accuracies vs neighbors is shown in Figure3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What impact could such findings have?\n",
    "\n",
    "The outcome from this study can inform the development of better education and study strategies to help educational institutions and students. Confirming that higher knowledge level is associated with higher exam performance on both goal objects and related objects, means that educational institutions may add more related materials to the goal objects in lectures and school. They can provide students extra links to related e-book or related videos, and hold extra experiments or presentations to deeper the understanding of the goal objects and broaden students' horizons of the goal objects. Students can also find a better way to gain higher scores in an exam, by not only looking at goal materials but also others beyond and related to goal objects.\n",
    "    \n",
    "Furthermore, this findings can provide some ideas to researchers in the field of education to create a more advanced and complete educational system to cultivate the next generation in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What future question could this lead to?\n",
    "\n",
    "  1. Though the accuracy of our k nearest neighbor classification model is relatively high(around 97.24%), are there variables outside the dataset could improve the prediction accuracy?\n",
    "  2. Is there a better algorithm we can use to analyze this dataset?\n",
    "  3. Can our classification model be applied to a general population?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "Dua, D. & Graff, C. (2019). <a href=\"http://archive.ics.uci.edu/ml\" target=\"_blank\">UCI Machine Learning Repository</a>. Irvine, CA: University of California, School of Information and Computer Science.\n",
    "\n",
    "Hartwig, M. K., Was, C. A., Isaacson, R. M., & Dunlosky, J. (2012). General knowledge monitoring as a predictor of in-class exam performance: <a href=\"https://doi.org/10.1111/j.2044-8279.2011.02038.x\" target =\"_blank\">General knowledge monitoring</a>. British Journal of Educational Psychology, 82(3), 456-468.\n",
    "\n",
    "Kahraman, H. T., Sagiroglu, S., & Colak, I. (2013). Developing intuitive knowledge classifier and modeling of users' domain dependent data in web, Knowledge Based Systems, vol. 37, 283-295.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
